# Speech Emotion Recognition System
## Project Overview
This project aims to recognize human emotions from speech using the **RAVDESS dataset**. The system processes audio features like **MFCC**, **Mel spectrogram**, and **Chroma** to classify emotions such as **Happy**, **Sad**, **Fear**, **Calm**, and **Anger**. A **Multi-Layer Perceptron (MLP)** model is used for classification based on the extracted features.

## Features
- **MFCC (Mel Frequency Cepstral Coefficients)**: Extracted from audio files for feature extraction.
- **Chroma Features**: Used to capture harmonic properties of the speech.
- **Mel-Spectrogram**: A representation of the sound spectrum that helps to understand speech patterns.

## Model
The emotion recognition model uses a **Multi-Layer Perceptron (MLP)** neural network. It takes the extracted audio features and classifies them into emotions based on patterns learned during training.


